{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.math.unipd.it/~marcuzzi/BannerStrumentifondamentali.png)\n",
    "\n",
    "# Moltiplicazione tra matrici\n",
    "\n",
    "In questo kernel vedremo come utilizzare la shared memory in un problema di algebra lineare. Date due matrici $A$ e $B$ di dimensione rispettivamente $m \\times k$ e $k \\times n$, vogliamo scrivere un kernel per calcolare il prodotto $C = AB$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda\n",
    "import math\n",
    "from timeit import default_timer\n",
    "from matplotlib import pyplot as plt\n",
    "#from matplotlib import colors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 128\n",
    "k = 256\n",
    "n = 128\n",
    "\n",
    "A = np.random.randint(10, size = (m,k), dtype = 'int32')\n",
    "B = np.random.randint(10, size = (k,n), dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo di esecuzione =  0.0034603364765644073 s\n"
     ]
    }
   ],
   "source": [
    "t_i  = default_timer()\n",
    "C_np = A@B\n",
    "t_f  = default_timer()\n",
    "print(\"Tempo di esecuzione = \", t_f - t_i, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_py(A,B):\n",
    "    if (B.shape[0] != A.shape[1]):\n",
    "        print(\"Errore: le dimensioni delle matrici non sono compatibili!\")\n",
    "        return\n",
    "    \n",
    "    C = np.zeros((m,n))\n",
    "    \n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            tmp = 0\n",
    "            for k in range(A.shape[1]):\n",
    "                tmp = tmp + A[i,k]*B[k,j]\n",
    "            #endfor\n",
    "            C[i,j] = tmp\n",
    "        #endfor\n",
    "    #endfor\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| C_np - C_py || 0.0\n",
      "Tempo di esecuzione =  1.8731305487453938 s\n"
     ]
    }
   ],
   "source": [
    "t_i  = default_timer()\n",
    "C_py = matmul_py(A,B)\n",
    "t_f  = default_timer()\n",
    "print('|| C_np - C_py ||', np.linalg.norm(C_np-C_py))\n",
    "print(\"Tempo di esecuzione = \", t_f - t_i, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella cella seguente troviamo un'implementazione naif di un prodotto tra matrici su GPU. Assegnamo al thread $(i,j)$ il calcolo dell'elemento \n",
    "\n",
    "$$C_{ij} = \\sum_k A_{i,k}B_{kj} = A_{i:} \\cdot B_{:j},$$\n",
    "\n",
    "che altro non è che il prodotto interno dell'$i$-esima riga di $A$, $A_{i:}$, e della $j$-esima colonna di $B$, $B_{:j}$. Senza tener conto della memoria shared, ogni thread legge una righa di $A$ e una colonna di $B$, come illustrato nella figura sotto. \n",
    "\n",
    "![](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-without-shared-memory.png)\n",
    "\n",
    "Notiamo che in questo modo la matrice $A$ viene letta $n$ (`B.width`) volte dalla global memory e la matrice $B$ viene letta $m$ (`A.height`) volte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_gpu(A,B,C):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        #endfor\n",
    "        C[i, j] = tmp\n",
    "    #endif\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| C_np - C_gpu || 0.0\n"
     ]
    }
   ],
   "source": [
    "TPB = 32\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = math.ceil(A.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(B.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "C_gpu = np.zeros((m,n), dtype = np.int32)\n",
    "\n",
    "matmul_gpu[blockspergrid, threadsperblock](A,B,C_gpu)\n",
    "print('|| C_np - C_gpu ||', np.linalg.norm(C_np-C_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo di esecuzione =  0.21100536733865738 s\n"
     ]
    }
   ],
   "source": [
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C_gpu = cuda.device_array_like(C_gpu)\n",
    "\n",
    "t_i = default_timer()\n",
    "matmul_gpu[blockspergrid, threadsperblock](d_A,d_B,d_C_gpu)\n",
    "cuda.synchronize()\n",
    "t_f = default_timer()\n",
    "print(\"Tempo di esecuzione = \", t_f - t_i, \"s\" )\n",
    "\n",
    "del  d_A, d_B, d_C_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared memory e tiling\n",
    "\n",
    "Vediamo l'implementazione con la memoria shared. Suddividiamo la matrice $C$ in sottomatrici quadrate $C_{sub}$.  Come illustrato nella figura sotto, la sottomatrice $C_{sub}$ è il prodotto di due sottomatrici rettangolari. Assegnamo ogni blocco di thread al calcolo di una sottomatrice quadrata $C_{sub}$ di $C$ e ogni thread all'interno del blocco al calcolo di un elemento di $C_{sub}$.\n",
    "Notiamo che così facendo, ogni blocco è indipendente dagli altri. Questo requisito è fondamentale se vogliamo implementare un algoritmo sulla GPU. Notiamo anche come a livello di blocchi si ha un palallelismo a grana grossa e come all'interno di ogni blocco si abbia un parallelismo a grana fine dato dai thread  paralleli. Nella memoria shared dovremo cacaricare la sottomatrice di $A$ di dimensione `(A.width, block_size)` e la sottomatrice di $B$ di dimensione `(block_size, A.width)`.\n",
    "La memoria shared è molto più piccola della memoria global, e le matrici potrebbero essere troppo grandi per essere caricate in shared, perciò è necessario suddividerle in un numero oppurtuno di sottomatrici quadrate di dimensione `(block_size, block_size)` dove `block_size`è la dimensione dei blocchi. $C_{sub}$ viene calcolata come una somma incrementale di prodotti di sottomatrici. Ognuno di questi prodotti viene effettuato caricando prima le corrispondenti sottomatrici quadrate di $A$ e $B$ in memoria shared (ogni thread si occupa di caricare un elemento della sottomatrice di $A$ e uno della sottomatrice di $B$), dopodiché ogni thread si occupa del calcolo di un elemento del prodotto. Ogni thread accumula i risultati in una somma parziale (una varialibe in memoria locale), e una volta terminate le piastrelle scrive il risultato nella memoria global. Questa tecnica prende il come di **tiling**, dall'inglese *tile*, \"piastrella\".\n",
    "\n",
    "![](https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png)\n",
    "\n",
    "Questo approccio ci permette di minimizzare il numero delle letture di $A$ e $B$ dalla memoria global: $A$ viene letta solo `(B.width / block_size)` volte e $B$ viene letta `(A.height / block_size)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPB = 32\n",
    "@cuda.jit()\n",
    "def matmul_gpu_shared(A, B, C):\n",
    "    # Controllo che le matrici A e B siano compatibili per il prodotto\n",
    "    n = A.shape[1]\n",
    "    if (B.shape[0] != n):\n",
    "        return\n",
    "    \n",
    "    # Definisco array shared per caricare le piastrelle\n",
    "    # La dimensione e il tipo devono essere noti al momento della compilazione\n",
    "    A_sh = cuda.shared.array(shape=(TPB, TPB), dtype=numba.int32)\n",
    "    B_sh = cuda.shared.array(shape=(TPB, TPB), dtype=numba.int32)\n",
    "    i, j = cuda.grid(2) #indici per accedere alla memoria globale\n",
    "\n",
    "    s_i = cuda.threadIdx.x #indice locale x per accedere alla memoria shared\n",
    "    s_j = cuda.threadIdx.y #indice locale y per accedere alla memoria shared\n",
    "    Ntiles = math.ceil(n/TPB) # numero di piastrelle lungo la direzione comune di A e B\n",
    "\n",
    "    # Ogni thread calcola un elemento della matrice C.\n",
    "    # Il prodotto interno tra le righe e le colonne viene spezzato nel prodotto interno di vettori di lunghezza TPB\n",
    "    if (i < C.shape[0]) and (j < C.shape[1]):\n",
    "        tmp = 0\n",
    "        for k in range(Ntiles):\n",
    "            # Caricamento dati in shared memory\n",
    "            if (s_j + k * TPB < n) and (s_i + k * TPB < n):\n",
    "                A_sh[s_i, s_j] = A[i, s_j + k * TPB]\n",
    "                B_sh[s_i, s_j] = B[s_i + k * TPB, j]\n",
    "            else:\n",
    "                A_sh[s_i, s_j] = 0\n",
    "                B_sh[s_i, s_j] = 0\n",
    "            #endif\n",
    "        \n",
    "            # Barriera: attendo che tutti i threads abbiano terminato il caricamento in shared\n",
    "            cuda.syncthreads()\n",
    "\n",
    "            # Calcolo il prodotto parziale nella shared memory\n",
    "            for l in range(TPB):\n",
    "                tmp += A_sh[s_i, l] * B_sh[l, s_j]\n",
    "\n",
    "            # Barriera: attendo che tutti i threads abbiano terminato il calcolo\n",
    "            cuda.syncthreads()\n",
    "        #endif\n",
    "    \n",
    "        #scrivo il risultato in global memory\n",
    "        C[i, j] = tmp\n",
    "    #endif\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| C_np - C_gpu || 0.0\n"
     ]
    }
   ],
   "source": [
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = math.ceil(A.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(B.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "matmul_gpu_shared[blockspergrid, threadsperblock](A,B,C_gpu)\n",
    "print('|| C_np - C_gpu ||', np.linalg.norm(C_np-C_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo di esecuzione =  0.2962923627346754 s\n"
     ]
    }
   ],
   "source": [
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C_gpu = cuda.device_array_like(C_np)\n",
    "\n",
    "t_i = default_timer()\n",
    "matmul_gpu_shared[blockspergrid, threadsperblock](d_A,d_B,d_C_gpu)\n",
    "cuda.synchronize()\n",
    "t_f = default_timer()\n",
    "print(\"Tempo di esecuzione = \", t_f - t_i, \"s\")\n",
    "\n",
    "del  d_A, d_B, d_C_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import guvectorize, void, int32\n",
    "\n",
    "@guvectorize(['int32[:,:], int32[:,:], int32[:,:]'], \n",
    "             '(m,k),(k,n)->(m,n)', target='cuda')\n",
    "def matmul_gu(A, B, C):\n",
    "    m, k = A.shape\n",
    "    k, n = B.shape\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            tmp = 0\n",
    "            for l in range(k):\n",
    "                tmp += A[i, l] * B[l, j]\n",
    "            #endfor\n",
    "            C[i, j] = tmp\n",
    "        #endfor\n",
    "    #endfor\n",
    "    return\n",
    " \n",
    "matmul_gu.max_blocksize = (32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| C_np - C_gu || 0.0\n",
      "Tempo di esecuzione =  0.30342525243759155 s\n"
     ]
    }
   ],
   "source": [
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C_gu = cuda.device_array_like(C_np)\n",
    "\n",
    "t_i = default_timer()\n",
    "matmul_gu(d_A, d_B, d_C_gu)\n",
    "cuda.synchronize()\n",
    "t_f = default_timer()\n",
    "C_gu = d_C_gu.copy_to_host()\n",
    "print('|| C_np - C_gu ||', np.linalg.norm(C_np - C_gu))\n",
    "print(\"Tempo di esecuzione = \", t_f - t_i, \"s\")\n",
    "\n",
    "del d_A, d_B, d_C_gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione matrici = 128 x 128\n",
      "Numpy: 0.00191385485231876 s\n",
      "CUDA senza shared memory: 0.00044769421219826 s\n",
      "CUDA con shared memory: 0.00047342479228973 s\n",
      "guvectoruze: 0.13595477305352688 s\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "k = 4\n",
    "n = TPB * k\n",
    "BPG = math.ceil(n/TPB)\n",
    "block_dim = (TPB, TPB)\n",
    "grid_dim = (BPG, BPG)\n",
    "\n",
    "# Prepare data on the CPU\n",
    "A = np.array(np.random.random((n, n)), dtype=np.int32, order = 'F')\n",
    "B = np.array(np.random.random((n, n)), dtype=np.int32, order = 'F')\n",
    "C = np.zeros_like(A)\n",
    "\n",
    "print(\"Dimensione matrici = %d x %d\" % (n, n))\n",
    "\n",
    "# Prepare data on the GPU\n",
    "dA = cuda.to_device(A)\n",
    "dB = cuda.to_device(B)\n",
    "dC = cuda.to_device(C) # device_array_like(A)\n",
    "\n",
    "# Time numpy version\n",
    "t_i = default_timer()\n",
    "np_ans = np.dot(A, B)\n",
    "t_f = default_timer()\n",
    "t_np = t_f - t_i \n",
    "\n",
    "# Time the unoptimized version\n",
    "t_i = default_timer()\n",
    "matmul_gpu[grid_dim, block_dim](dA, dB, dC)\n",
    "cuda.synchronize()\n",
    "t_f = default_timer()\n",
    "tcuda = t_f - t_i \n",
    "\n",
    "\n",
    "# Time the shared memory version\n",
    "t_i = default_timer()\n",
    "matmul_gpu_shared[grid_dim, block_dim](dA, dB, dC)\n",
    "cuda.synchronize()\n",
    "t_f = default_timer()\n",
    "tcuda_sh = t_f - t_i \n",
    "\n",
    "# Time guvectorize version\n",
    "t_i = default_timer()\n",
    "matmul_gu(dA, dB, dC)\n",
    "cuda.synchronize()\n",
    "t_f = default_timer()\n",
    "t_gu = t_f - t_i \n",
    "\n",
    "print(\"Numpy:\", \"%.17f\" % t_np, \"s\")\n",
    "print(\"CUDA senza shared memory:\", \"%.17f\" % tcuda, \"s\")\n",
    "print(\"CUDA con shared memory:\", \"%.17f\" % tcuda_sh, \"s\")\n",
    "print(\"guvectoruze:\", \"%.17f\" % t_gu, \"s\")\n",
    "\n",
    "del dA, dB, dC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
